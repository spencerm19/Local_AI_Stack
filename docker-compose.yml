volumes:
  n8n_storage:
  ollama_storage:
  open-webui:
  flowise:
  valkey-data:
  supabase_db_data:
  supabase_storage_data:
  supabase_kong_data:
  supabase_redis_data:

networks:
  Systems-AI:
    driver: bridge

x-n8n: &service-n8n
  image: n8nio/n8n:1.24.1
  environment:
    - DB_TYPE=postgresdb
    - DB_POSTGRESDB_HOST=db
    - DB_POSTGRESDB_USER=postgres
    - DB_POSTGRESDB_PASSWORD=${POSTGRES_PASSWORD}
    - DB_POSTGRESDB_DATABASE=postgres
    - N8N_DIAGNOSTICS_ENABLED=false
    - N8N_PERSONALIZATION_ENABLED=false
    - N8N_ENCRYPTION_KEY=${N8N_ENCRYPTION_KEY}
    - N8N_USER_MANAGEMENT_JWT_SECRET=${N8N_JWT_SECRET}
    - N8N_SECURE_COOKIE=false
    - N8N_PROTOCOL=http
    - NODE_ENV=production
  networks:
    - Systems-AI

x-ollama: &service-ollama
  image: ollama/ollama:0.1.29
  restart: unless-stopped
  volumes:
    - ollama_storage:/root/.ollama
  environment:
    - OLLAMA_KEEP_ALIVE=24h
    - OLLAMA_HOST=0.0.0.0
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:11434/api/version"]
    interval: 30s
    timeout: 10s
    retries: 5
    start_period: 30s
  networks:
    - Systems-AI

services:
  # Database
  db:
    image: supabase/postgres:15.1.0.117
    restart: unless-stopped
    volumes:
      - supabase_db_data:/var/lib/postgresql/data
    environment:
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB:-postgres}
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
    networks:
      - Systems-AI
    command:
      - postgres
      - -c
      - config_file=/etc/postgresql/postgresql.conf
      - -c
      - log_min_messages=fatal
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres} -d ${POSTGRES_DB:-postgres}"]
      interval: 10s
      timeout: 5s
      retries: 5
    depends_on:
      ollama-cpu:
        condition: service_healthy
        required: false
      ollama-gpu:
        condition: service_healthy
        required: false
      ollama-gpu-amd:
        condition: service_healthy
        required: false

  # Meta API
  meta:
    image: supabase/postgres-meta:v0.75.0
    restart: unless-stopped
    environment:
      PG_META_PORT: 8080
      PG_META_DB_HOST: db
      PG_META_DB_PASSWORD: ${POSTGRES_PASSWORD}
      PG_META_DB_USER: ${POSTGRES_USER:-postgres}
      PG_META_DB_NAME: ${POSTGRES_DB:-postgres}
    networks:
      - Systems-AI
    depends_on:
      db:
        condition: service_healthy

  # Kong Gateway
  kong:
    image: kong:3.5.0
    restart: unless-stopped
    ports:
      - "${KONG_HTTP_PORT:-8000}:8000/tcp"
      - "${KONG_HTTPS_PORT:-8443}:8443/tcp"
    volumes:
      - supabase_kong_data:/var/lib/kong
    environment:
      KONG_DATABASE: "off"
      KONG_DECLARATIVE_CONFIG: /var/lib/kong/kong.yml
      KONG_DNS_ORDER: LAST,A,CNAME
      KONG_PLUGINS: request-transformer,cors,key-auth,acl
    networks:
      - Systems-AI
    depends_on:
      db:
        condition: service_healthy

  # Auth API
  auth:
    image: supabase/auth:v2.139.1
    restart: unless-stopped
    environment:
      GOTRUE_API_HOST: 0.0.0.0
      GOTRUE_API_PORT: 9999
      API_EXTERNAL_URL: ${API_EXTERNAL_URL:-http://localhost:8000}
      DATABASE_URL: postgres://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB:-postgres}?sslmode=disable
      GOTRUE_DB_DRIVER: postgres
      GOTRUE_SITE_URL: ${SITE_URL:-http://localhost:3000}
      GOTRUE_URI_ALLOW_LIST: ${ADDITIONAL_REDIRECT_URLS}
      GOTRUE_DISABLE_SIGNUP: ${DISABLE_SIGNUP:-false}
      JWT_SECRET: ${JWT_SECRET:-your-super-secret-jwt-token-with-at-least-32-characters-long}
      GOTRUE_JWT_SECRET: ${JWT_SECRET:-your-super-secret-jwt-token-with-at-least-32-characters-long}
      GOTRUE_JWT_EXP: ${JWT_EXPIRY:-3600}
      GOTRUE_JWT_DEFAULT_GROUP_NAME: authenticated
      MAILER_AUTOCONFIRM: ${ENABLE_EMAIL_AUTOCONFIRM:-true}
      MAILER_SECURE_EMAIL_CHANGE_ENABLED: ${ENABLE_EMAIL_CHANGE:-true}
      MAILER_URLPATHS_INVITE: /auth/v1/verify
      MAILER_URLPATHS_CONFIRMATION: /auth/v1/verify
      MAILER_URLPATHS_RECOVERY: /auth/v1/verify
      MAILER_URLPATHS_EMAIL_CHANGE: /auth/v1/verify
    networks:
      - Systems-AI
    depends_on:
      db:
        condition: service_healthy

  # REST API
  rest:
    image: postgrest/postgrest:v12.0.2
    restart: unless-stopped
    environment:
      PGRST_DB_URI: postgres://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB:-postgres}
      PGRST_DB_SCHEMA: public,storage
      PGRST_DB_ANON_ROLE: anon
      PGRST_JWT_SECRET: ${JWT_SECRET:-your-super-secret-jwt-token-with-at-least-32-characters-long}
      PGRST_DB_USE_LEGACY_GUCS: "false"
    networks:
      - Systems-AI
    depends_on:
      db:
        condition: service_healthy

  # Realtime API
  realtime:
    image: supabase/realtime:v2.25.50
    restart: unless-stopped
    environment:
      PORT: 4000
      DB_HOST: db
      DB_PORT: 5432
      DB_USER: ${POSTGRES_USER:-postgres}
      DB_PASSWORD: ${POSTGRES_PASSWORD}
      DB_NAME: ${POSTGRES_DB:-postgres}
      DB_AFTER_CONNECT_QUERY: 'SET search_path TO _realtime'
      DB_ENC_KEY: supabaserealtime
      API_JWT_SECRET: ${JWT_SECRET:-your-super-secret-jwt-token-with-at-least-32-characters-long}
      FLY_ALLOC_ID: fly123
      FLY_APP_NAME: realtime
      SECRET_KEY_BASE: UpNVntn3cDxHJpq99YMc1T1AQgQpc8kfYxHE1U5P+h2osuOE8oHdblzwZHOo4f35
      ERL_AFLAGS: -proto_dist inet_tcp
      ENABLE_TAILSCALE: "false"
      DNS_NODES: "''"
    networks:
      - Systems-AI
    depends_on:
      db:
        condition: service_healthy

  # Redis (for rate limiting)
  redis:
    image: redis:7.2.4
    restart: unless-stopped
    volumes:
      - supabase_redis_data:/data
    networks:
      - Systems-AI

  # Storage API
  storage:
    image: supabase/storage-api:v0.43.11
    restart: unless-stopped
    environment:
      ANON_KEY: eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJyb2xlIjoiYW5vbiIsImlhdCI6MTYyNzIwNzUzMiwiZXhwIjoxOTc0MzYxNTMyfQ.eSq3o3ZTyWe6xc9r1VYHPMpACaH_9vWTqwzL0RA2UZI
      SERVICE_KEY: eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJyb2xlIjoic2VydmljZV9yb2xlIiwiaWF0IjoxNjI3MjA3NTMyLCJleHAiOjE5NzQzNjE1MzJ9.M4hrAUDOP-S2qXF_shrb_v3qm5TI4Hs0n_QHMhPzvkg
      PROJECT_REF: ${PROJECT_REF:-default}
      POSTGREST_URL: http://rest:3000
      PGRST_JWT_SECRET: ${JWT_SECRET:-your-super-secret-jwt-token-with-at-least-32-characters-long}
      DATABASE_URL: postgres://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB:-postgres}?sslmode=disable
      FILE_SIZE_LIMIT: 52428800
      STORAGE_BACKEND: file
      FILE_STORAGE_BACKEND_PATH: /var/lib/storage
      TENANT_ID: stub
      REGION: ${STORAGE_REGION:-us-east-1}
      GLOBAL_S3_BUCKET: ${STORAGE_S3_BUCKET:-storage}
      ENABLE_IMAGE_TRANSFORMATION: "true"
      IMGPROXY_URL: http://imgproxy:5001
    volumes:
      - supabase_storage_data:/var/lib/storage
    networks:
      - Systems-AI
    depends_on:
      db:
        condition: service_healthy
      rest:
        condition: service_started
      imgproxy:
        condition: service_started

  # Image Transformation
  imgproxy:
    image: darthsim/imgproxy:v3.21
    restart: unless-stopped
    environment:
      IMGPROXY_BIND: ":5001"
      IMGPROXY_LOCAL_FILESYSTEM_ROOT: /
      IMGPROXY_USE_ETAG: "true"
    networks:
      - Systems-AI

  # Studio API
  studio:
    image: supabase/studio:v0.1.9
    restart: unless-stopped
    ports:
      - "${STUDIO_PORT:-3020}:3000"
    environment:
      STUDIO_PG_META_URL: http://meta:8080
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      DEFAULT_ORGANIZATION_NAME: ${ORGANIZATION_NAME:-Default Organization}
      DEFAULT_PROJECT_NAME: ${PROJECT_NAME:-Default Project}
      SUPABASE_PUBLIC_URL: ${PUBLIC_URL:-http://localhost:8000}
      SUPABASE_ANON_KEY: eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJyb2xlIjoiYW5vbiIsImlhdCI6MTYyNzIwNzUzMiwiZXhwIjoxOTc0MzYxNTMyfQ.eSq3o3ZTyWe6xc9r1VYHPMpACaH_9vWTqwzL0RA2UZI
      SUPABASE_SERVICE_KEY: eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJyb2xlIjoic2VydmljZV9yb2xlIiwiaWF0IjoxNjI3MjA3NTMyLCJleHAiOjE5NzQzNjE1MzJ9.M4hrAUDOP-S2qXF_shrb_v3qm5TI4Hs0n_QHMhPzvkg
    networks:
      - Systems-AI
    depends_on:
      db:
        condition: service_healthy
      meta:
        condition: service_started

  # n8n Workflow Automation
  n8n:
    <<: *service-n8n
    container_name: n8n
    restart: unless-stopped
    ports:
      - "5678:5678"
    volumes:
      - n8n_storage:/home/node/.n8n
      - ./shared:/data/shared
    environment:
      - WEBHOOK_TUNNEL_URL=${N8N_EDITOR_BASE_URL}
      - N8N_SECURE_COOKIE=false
      - N8N_PROTOCOL=http
    depends_on:
      db:
        condition: service_healthy
      ollama-cpu:
        condition: service_healthy
        required: false
      ollama-gpu:
        condition: service_healthy
        required: false
      ollama-gpu-amd:
        condition: service_healthy
        required: false

  # Flow-based Programming Tool
  flowise:
    image: flowiseai/flowise
    restart: unless-stopped
    container_name: flowise
    environment:
      - PORT=3001
      - HOST=0.0.0.0
      - FLOWISE_USERNAME=${FLOWISE_USERNAME:-admin}
      - FLOWISE_PASSWORD=${FLOWISE_PASSWORD:-password}
      - DEBUG=true
      - DATABASE_PATH=/root/.flowise
      - APIKEY_PATH=/root/.flowise
      - SECRETKEY_PATH=/root/.flowise
      - FLOWISE_SECRETKEY_OVERWRITE=${FLOWISE_SECRET_KEY:-Freyaai}
      - CORS_ORIGINS=${FLOWISE_URL:-http://localhost:3001}
      - EXECUTION_MODE=main
    ports:
      - "${FLOWISE_PORT:-3001}:3001"
    volumes:
      - flowise:/root/.flowise
    networks:
      - Systems-AI
    depends_on:
      ollama-cpu:
        condition: service_healthy
        required: false
      ollama-gpu:
        condition: service_healthy
        required: false
      ollama-gpu-amd:
        condition: service_healthy
        required: false

  # Web UI for Ollama
  open-webui-cpu:
    profiles: ["cpu"]
    image: ghcr.io/open-webui/open-webui:main
    restart: unless-stopped
    container_name: open-webui
    ports:
      - "${WEBUI_PORT:-3000}:8080"
    environment:
      - OLLAMA_BASE_URLS=http://ollama-cpu:11434
      - WEBUI_AUTH=${WEBUI_AUTH:-true}
      - WEBUI_NAME=Freya AI
      - WEBUI_URL=${WEBUI_URL:-http://localhost:3000}
      - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY}
    volumes:
      - open-webui:/app/backend/data
    networks:
      - Systems-AI
    depends_on:
      ollama-cpu:
        condition: service_healthy

  open-webui-gpu:
    profiles: ["gpu-nvidia", "gpu-amd"]
    image: ghcr.io/open-webui/open-webui:main
    restart: unless-stopped
    container_name: open-webui
    ports:
      - "${WEBUI_PORT:-3000}:8080"
    environment:
      - OLLAMA_BASE_URLS=http://ollama-gpu:11434,http://ollama-gpu-amd:11434
      - WEBUI_AUTH=${WEBUI_AUTH:-true}
      - WEBUI_NAME=Freya AI
      - WEBUI_URL=${WEBUI_URL:-http://localhost:3000}
      - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY}
    volumes:
      - open-webui:/app/backend/data
    networks:
      - Systems-AI
    depends_on:
      ollama-gpu:
        condition: service_healthy
      ollama-gpu-amd:
        condition: service_healthy

  # Ollama CPU Service
  ollama-cpu:
    <<: *service-ollama
    profiles: ["cpu"]
    container_name: ollama-cpu
    ports:
      - "${OLLAMA_PORT:-11434}:11434"

  # Ollama GPU Service (NVIDIA)
  ollama-gpu:
    <<: *service-ollama
    profiles: ["gpu-nvidia"]
    container_name: ollama-gpu
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Ollama GPU Service (AMD)
  ollama-gpu-amd:
    <<: *service-ollama
    profiles: ["gpu-amd"]
    container_name: ollama-gpu-amd
    image: ollama/ollama:rocm
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    devices:
      - /dev/kfd:/dev/kfd
      - /dev/dri:/dev/dri

  # Model initialization services
  ollama-init-cpu:
    profiles: ["cpu"]
    image: ollama/ollama:latest
    container_name: ollama-init-cpu
    volumes:
      - ollama_storage:/root/.ollama
    entrypoint: /bin/sh
    command:
      - "-c"
      - |
        echo "Waiting for Ollama to be ready...";
        while ! curl -s http://ollama-cpu:11434/api/version > /dev/null; do
          sleep 2;
        done;
        echo "Ollama is ready. Starting model downloads...";
        IFS=',' read -ra MODELS <<< "${OLLAMA_MODELS}";
        for MODEL in "$${MODELS[@]}"; do
          echo "Pulling model: $$MODEL";
          OLLAMA_HOST=ollama-cpu:11434 ollama pull $$MODEL;
        done
    depends_on:
      ollama-cpu:
        condition: service_healthy
    networks:
      - Systems-AI

  ollama-init-gpu:
    profiles: ["gpu-nvidia"]
    image: ollama/ollama:latest
    container_name: ollama-init-gpu
    volumes:
      - ollama_storage:/root/.ollama
    entrypoint: /bin/sh
    command:
      - "-c"
      - |
        echo "Waiting for Ollama to be ready...";
        while ! curl -s http://ollama-gpu:11434/api/version > /dev/null; do
          sleep 2;
        done;
        echo "Ollama is ready. Starting model downloads...";
        IFS=',' read -ra MODELS <<< "${OLLAMA_MODELS}";
        for MODEL in "$${MODELS[@]}"; do
          echo "Pulling model: $$MODEL";
          OLLAMA_HOST=ollama-gpu:11434 ollama pull $$MODEL;
        done
    depends_on:
      ollama-gpu:
        condition: service_healthy
    networks:
      - Systems-AI

  ollama-init-gpu-amd:
    profiles: ["gpu-amd"]
    image: ollama/ollama:rocm
    container_name: ollama-init-gpu-amd
    volumes:
      - ollama_storage:/root/.ollama
    entrypoint: /bin/sh
    command:
      - "-c"
      - |
        echo "Waiting for Ollama to be ready...";
        while ! curl -s http://ollama-gpu-amd:11434/api/version > /dev/null; do
          sleep 2;
        done;
        echo "Ollama is ready. Starting model downloads...";
        IFS=',' read -ra MODELS <<< "${OLLAMA_MODELS}";
        for MODEL in "$${MODELS[@]}"; do
          echo "Pulling model: $$MODEL";
          OLLAMA_HOST=ollama-gpu-amd:11434 ollama pull $$MODEL;
        done
    depends_on:
      ollama-gpu-amd:
        condition: service_healthy
    networks:
      - Systems-AI

